{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing-in-Buenos-Aires\n",
    "Starter notebook for the Housing-in-Buenos-Aires project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82dff8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Starter Notebook: Housing-in-Buenos-Aires Analysis\n",
    "# =========================================================\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from category_encoders import OneHotEncoder\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e9cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample housing data created at: /Users/ayoub.elfilali/Desktop/Housing-in-Buenos-Aires/data/raw/housing_data.csv\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.expanduser(\"~/Desktop/Housing-in-Buenos-Aires/data/raw\"), exist_ok=True)\n",
    "\n",
    "# Sample data for Buenos Aires apartments\n",
    "data = {\n",
    "    \"place_with_parent_names\": [\n",
    "        \"Argentina|Buenos Aires|Palermo\",\n",
    "        \"Argentina|Buenos Aires|Recoleta\",\n",
    "        \"Argentina|Buenos Aires|Palermo\",\n",
    "        \"Argentina|Buenos Aires|Caballito\",\n",
    "        \"Argentina|Buenos Aires|Belgrano\"\n",
    "    ],\n",
    "    \"property_type\": [\"apartment\", \"apartment\", \"apartment\", \"apartment\", \"apartment\"],\n",
    "    \"price_aprox_usd\": [95000, 87000, 99000, 92000, 88000],\n",
    "    \"surface_covered_in_m2\": [55, 45, 60, 50, 48],\n",
    "    \"lat-lon\": [\"-34.58,-58.41\", \"-34.59,-58.38\", \"-34.57,-58.42\", \"-34.60,-58.44\", \"-34.56,-58.39\"],\n",
    "    \"surface_total_in_m2\": [60, 50, 65, 55, 53],\n",
    "    \"price_usd_per_m2\": [1727, 1933, 1650, 1840, 1830],\n",
    "    \"floor\": [3, 2, 5, 4, 2],\n",
    "    \"rooms\": [2, 1, 3, 2, 1],\n",
    "    \"expenses\": [200, 150, 250, 180, 160],\n",
    "    \"operation\": [\"sale\", \"sale\", \"sale\", \"sale\", \"sale\"],\n",
    "    \"currency\": [\"USD\", \"USD\", \"USD\", \"USD\", \"USD\"],\n",
    "    \"properati_url\": [\"url1\", \"url2\", \"url3\", \"url4\", \"url5\"],\n",
    "    \"price\": [95000, 87000, 99000, 92000, 88000],\n",
    "    \"price_aprox_local_currency\": [0,0,0,0,0],\n",
    "    \"price_per_m2\": [0,0,0,0,0]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.expanduser(\"~/Desktop/Housing-in-Buenos-Aires/data/raw/housing_data.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Sample housing data created at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c80aab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Paths\n",
    "# =========================================================\n",
    "RAW_DATA_PATH = \"../data/raw\"\n",
    "OUTPUT_PATH = \"../outputs\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6efee294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: []\n",
      "No CSV files found in the folder.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Load all housing CSVs\n",
    "# =========================================================\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Get all CSV files matching the pattern\n",
    "all_files = glob(\"data/raw/*.csv\")\n",
    "print(\"Files found:\", all_files)\n",
    "\n",
    "dfs = []\n",
    "for file in all_files:\n",
    "    df_file = pd.read_csv(file)\n",
    "    dfs.append(df_file)\n",
    "\n",
    "# Only concatenate if thereâ€™s at least one DataFrame\n",
    "if dfs:\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Loaded {len(all_files)} files, total rows: {len(df)}\")\n",
    "else:\n",
    "    print(\"No CSV files found in the folder.\")\n",
    "    df = pd.DataFrame()  # create empty DataFrame to avoid breaking the rest of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f3d3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Wrangle function\n",
    "# =========================================================\n",
    "def wrangle(df, city_name=\"Buenos Aires\"):\n",
    "    # Filter apartments in city < 100,000 USD\n",
    "    mask_city = df[\"place_with_parent_names\"].str.contains(city_name, na=False)\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 100_000\n",
    "    df = df[mask_city & mask_apt & mask_price].copy()\n",
    "\n",
    "    # Remove outliers for covered area\n",
    "    low, high = df[\"surface_covered_in_m2\"].quantile([0.1, 0.9])\n",
    "    df = df[df[\"surface_covered_in_m2\"].between(low, high)]\n",
    "\n",
    "    # Split lat-lon column\n",
    "    df[[\"lat\", \"lon\"]] = df[\"lat-lon\"].str.split(\",\", expand=True).astype(float)\n",
    "    df.drop(columns=\"lat-lon\", inplace=True)\n",
    "\n",
    "    # Extract borough\n",
    "    df[\"borough\"] = df[\"place_with_parent_names\"].str.split(\"|\", expand=True)[1]\n",
    "    df.drop(columns=\"place_with_parent_names\", inplace=True)\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    drop_cols = [\"surface_total_in_m2\", \"price_usd_per_m2\", \"floor\", \"rooms\", \n",
    "                 \"expenses\", \"operation\", \"property_type\", \"currency\", \n",
    "                 \"properati_url\", \"price\", \"price_aprox_local_currency\", \n",
    "                 \"price_per_m2\"]\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fba306ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Housing-in-Buenos-Aires/data/raw/housing_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Continue with the rest of your wrangling...\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHousing-in-Buenos-Aires/data/raw/housing_data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData wrangled successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m display(df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Housing-in-Buenos-Aires/data/raw/housing_data.csv'"
     ]
    }
   ],
   "source": [
    "# Apply wrangling\n",
    "def wrangle(df, city_name=\"Buenos Aires\"):\n",
    "    # Example: adjust column names to match your CSV\n",
    "    mask_city = df[\"place_name\"].str.contains(city_name, na=False)\n",
    "    mask_apt = df[\"property_type\"] == \"apartment\"\n",
    "    mask_price = df[\"price_aprox_usd\"] < 100_000\n",
    "    df = df[mask_city & mask_apt & mask_price]\n",
    "    # Continue with the rest of your wrangling...\n",
    "    return df\n",
    "df = pd.read_csv(\"Housing-in-Buenos-Aires/data/raw/housing_data.csv\")\n",
    "\n",
    "print(\"Data wrangled successfully\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de18f9c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['surface_covered_in_m2', 'lat', 'lon', 'borough'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m target = \u001b[33m\"\u001b[39m\u001b[33mprice_aprox_usd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m features = [\u001b[33m\"\u001b[39m\u001b[33msurface_covered_in_m2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlon\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mborough\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m y = df[target]\n\u001b[32m     10\u001b[39m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[32m     11\u001b[39m     X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WQU-DataScienceLab/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['surface_covered_in_m2', 'lat', 'lon', 'borough'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Train-test split\n",
    "# =========================================================\n",
    "target = \"price_aprox_usd\"\n",
    "features = [\"surface_covered_in_m2\", \"lat\", \"lon\", \"borough\"]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Baseline model: Mean predictor\n",
    "# =========================================================\n",
    "y_mean = y_train.mean()\n",
    "y_pred_baseline = [y_mean] * len(y_train)\n",
    "baseline_mae = mean_absolute_error(y_train, y_pred_baseline)\n",
    "print(f\"Baseline MAE (train): {baseline_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a736c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Ridge regression pipeline\n",
    "# =========================================================\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    SimpleImputer(),\n",
    "    Ridge()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccabffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3638b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_test_pred = pd.Series(model.predict(X_test), index=y_test.index)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "print(f\"Ridge regression MAE (test): {mae_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Feature importance\n",
    "# =========================================================\n",
    "intercept = model.named_steps[\"ridge\"].intercept_\n",
    "coefficients = model.named_steps[\"ridge\"].coef_\n",
    "features_encoded = model.named_steps[\"onehotencoder\"].get_feature_names()\n",
    "\n",
    "feat_imp = pd.Series(coefficients, index=features_encoded).sort_values(key=abs, ascending=False)\n",
    "print(\"Top feature importances:\")\n",
    "display(feat_imp.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ed1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# Quick plots\n",
    "# =========================================================\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df[target], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Property Prices\")\n",
    "plt.xlabel(\"Price (USD)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, \"price_distribution.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance plot\n",
    "plt.figure(figsize=(12,6))\n",
    "feat_imp.head(10).plot(kind=\"barh\")\n",
    "plt.title(\"Top 10 Feature Importances (Ridge Regression)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, \"feature_importance.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
